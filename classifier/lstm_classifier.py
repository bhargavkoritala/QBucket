# -*- coding: utf-8 -*-
"""lstm_classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KQp1gFNbnrjTVmNDMrBddLRrdaRPWQOv
"""

import numpy as np 
import pandas as pd
import re
import gc
import os
import fileinput
import string
import tensorflow as tf
import zipfile
import datetime
import sys
from tqdm  import tqdm
tqdm.pandas()
from nltk.tokenize import wordpunct_tokenize
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score, roc_auc_score

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation
from keras.layers.embeddings import Embedding
from sklearn.metrics import classification_report

def detect(tokens):
    return [t for t in tokens if t in valid_forms]
    
def replace_blank(tokens):
    return [blank if t in valid_forms else t for t in tokens]

def create_windows(tokens, window_size=3):
    X = []
    for i, word in enumerate(tokens):
        if word == blank:
            window = tokens[i-window_size:i] + tokens[i+1:i+window_size+1]
            window = ' '.join(window)
            X.append(window)    
    return X

destination_folder = '/content/drive/My Drive/transformers/Model'
from google.colab import drive
drive.mount('/content/drive')

#Pre-processing of the data

raw_data_path = './train.csv'
destination_folder = ''

train_test_ratio = 0.10
train_valid_ratio = 0.80

first_n_words = 200

import pandas as pd
from sklearn.model_selection import train_test_split

def trim_string(x):

    x = x.split(maxsplit=first_n_words)
    x = ' '.join(x[:first_n_words])

    return x

# Read raw data
df_raw = pd.read_csv(raw_data_path)
df_raw = df_raw.drop(['Option1', 'Option2', 'Option3', 'Option4'], axis = 1).dropna()
print(df_raw['Label'].unique())
df_raw.head()

df_raw['Label'] = df_raw['Label'].map({'biology': 0, 'physics': 1, 'chemistry': 2, 'geology':3})

# Trim text and titletext to first_n_words
df_raw['Text'] = df_raw['Text'].apply(trim_string)
df = df_raw[['Text']]
df.head()

df["Label"] = LabelEncoder().fit_transform(df_raw['Label'])
df.head()

one_hot = pd.get_dummies(df["Label"])
df.drop(['Label'],axis=1,inplace=True)
df = pd.concat([df,one_hot],axis=1)
df.head()

# Train-test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df["Text"].values, df.drop(['Text'],axis=1).values, test_size=0.2, random_state=42)
print(X_train[:10], y_train[:10])

vocabulary_size = 2000
tokenizer = Tokenizer(num_words= vocabulary_size)
tokenizer.fit_on_texts(X_train)
sequences = tokenizer.texts_to_sequences(X_train)
X_train = pad_sequences(sequences, maxlen=50)

sequences = tokenizer.texts_to_sequences(X_test)
X_test = pad_sequences(sequences, maxlen=50)

model = Sequential()
model.add(Embedding(2000, 100, input_length=50))
model.add(Dropout(0.2))
model.add(Conv1D(64, 5, activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(LSTM(40))
model.add(Dense(4, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(X_train, y_train,
                    batch_size=256,
                    epochs=50,
                    verbose=1,
                    validation_split=0.2)

model.save("LSTM_Model.h5")

score = model.evaluate(X_test, y_test,
                       batch_size=256, verbose=1)
print('Test accuracy:', score[1])

preds = model.predict(X_test)

print(classification_report(np.argmax(y_test,axis=1),np.argmax(preds,axis=1)))

